# This project is to demonstrate  how to implement the
# mathematical theory of linear regression into python.
# Using the book_reviews dataset from
# Alexander Holden

import pandas as pd
import matplotlib.pyplot as plt

data = pd.read_csv('score.csv')

# Data Visualisation
# randomly scatters data on a graph
#plt.scatter(data.studytime, data.score)
# shows graph
#plt.show()


# Calculates and returns the total squared error of a simple linear regression model
# A loss_function is a method of determining the quality of a prediction.
# The higher the returned value from total_error, the more off my prediction is, vise versa.
# The datapoints variable represents the data that I'm feeding the algorithm.
def loss_function(m, b, datapoints):
    total_error = 0
    # By default, the total_error is 0 so that I can ensure that I'm not adding extra errors before I start calculating.

    # This section generates a sequence of numbers from 0 to the length of 'datapoints'
    # Generates a FOR loop where 'i' takes on the next value in the sequence generated by range(len(datapoints))
    for i in range(len(datapoints)):
        # Uses the iloc (integer location) function from the pandas library.
        # Accesses specific rows associated with the x/y attributes within the dataframe (2x2 array function)
        # [i] is used to select the i-th row.
        # The i variable represents an index - a value that allows us to locate elements in an array (for example, an id attached to a product in a store)
        # i is like a counter keeping track of which product you’re currently looking at as you iterate through the list.
        x = datapoints.iloc[i].studytime
        y = datapoints.iloc[i].score

        # The += operator in Python is shorthand for “add the right-hand side to the left-hand side”. It's used to accumulate the total error of the LR model. Adds the squared difference from (**2) into the running total of teh error.
        # (y - (m * x + b)) ** 2 calculates the difference between the actual y-value and the y-value predicted by the model for the given x-value.
        # (m * x + b) is the prediction. Here, m is the slope of the line, b is the y-intercept, and x is the input.
        # After stating the prediction, I have to square it (** 2) to make sure all differences are positive (because negative differences would cancel out positive ones).
        total_error += (y - (m * x + b)) ** 2

        # len(datapoints) gives the total number of data points in the dataset.
        # Float converts the number of datapoints to a floating point data type. Float is simply a positive or negative whole number with a decimal point e.g (-)23.5
        # total_error /   divides the total error by the number of data points.
        total_error / float(len(datapoints))


# Define function called gradient_descent with four parameters (variable used to refer to input data)
# m_now is the current(now) slope of the line. How steep is the line? Slope = Rise / Run, change in y / change in x
# b_now is the current(now) y-intercept of the line.
# Datapoints is a DataFrame (2x2 array) containing the datapoints (locations or iloc (location of i within the dataframe))
# Learning_rate is a parameter that controls how much the slope and y-intercept are adjusted in each step of teh algorithm
# m/b_gradient lines initialize / instantiate the variables that will hold the gradients (the slopes of the error function) with respect to m and b.
def gradient_descent(m_now, b_now, datapoints, learning_rate):
    m_gradient = 0
    b_gradient = 0

    # This line calculates the number of data points
    n = len(datapoints)

    # This for loop iterates over each datapoint in the DataFrame where 'n' represents the range, or how many times the loop should iterate.
    for i in range(n):
        # Uses the iloc (integer location) function from the pandas library.
        # Accesses specific rows associated with the x/y attributes within the dataframe (2x2 array function)
        # [i] is used to select the i-th row.
        # The i variable represents an index - a value that allows us to locate elements in an array (for example, an id attached to a product in a store)
        # i is like a counter keeping track of which product you’re currently looking at as you iterate through the list.
        x = datapoints.iloc[i].studytime
        y = datapoints.iloc[i].score

        # m_gradient is a variable that stores the gradient of the loss function with respect to m, which represents the slope of the line.
        # += is an operation that increases the value of m_gradient by the value of the expression on the right.
        # 2/n is used to scale the sum of squared errors when updating the parameters.
        # This helps to ensure that the updates to your parameters don’t get too large, especially when dealing with large datasets.
        # The - sign  ensures that we’re moving in the direction that decreases our loss function.
        # n/2 = total number of datapoints (n) divided by 2 (symbol for squared)
        # x * x represents the square of the x-coordinate of a data point.
        # y is the actual value of the dependent variable for a given data point.
        # m_now * x + b_now is the predicted value of the dependent variable for the same data point
        # m_now = slope of the line, b_now = y-intercept, x = independent variable (the input)
        m_gradient += -(2 / n) * x * x * (y - (m_now * x + b_now))
        b_gradient += -(2 / n) * x * (y - (m_now * x + b_now))

        # m = current value of m or b(m_now/b_now) -how much the value of the loss_function changes when m/b is updated by m_now *  how big of a step we take when updating m/b
        m = m_now - m_gradient * learning_rate
        b = b_now - b_gradient * learning_rate

        # Returns newly updated m/b values
        return m, b

    # Initialize m and b to 0
    m = 0
    b = 0

    # The learning rate is set to a small value so that the algorithm takes very small steps each iteration, minimising the rate of error.
    learning_rate = 0.0001

    # Total number of iterations(epochs) of all the training data in one cycle for training the machine learning model
    epochs = 1000

    # This line starts a loop that will run for a number of times equal to iterations. Each iteration of the loop represents a complete pass through the datapoints (data)
    for i in range(epochs):
        # This line checks if the current iteration number (i) is a multiple of 50. % returns the remainder of the division of i by 50. If the remainder is 0, that means i is a multiple of 50
        if i % 50 == 0:
            # Print Iteration number every 50 iterations
            print(f"Iterations: {i}")

        # Calls the function that I just created (gradient_descent) and uses the current values of m/b.
        # The updated values of m and b are then returned by the function and assigned back to m and b, ready for the next iteration.
        m, b = gradient_descent(m, b, datapoints, learning_rate)

    # Data Visualisation
    # Print values of m/b (slope & y-intercept)
    print(m, b)

    # Create scatter plot graph with the coordinates for studytime(x) and score(y) using the matplotlib library
    plt.scatter(data.studytime, data.score, color="black")

    # Plots a line on the graph that represents the linear model defined by m and b. X is between 1 and 99.
    # Y coordinates are calculated by y=m*x+b, which calculates the location of y using x
    plt.plot(list(range(1, 300)), [m * x + b for x in range(1, 300)], color="red")

    plt.show()
